{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install twscrape\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "credentials = './fajarshiddiqqq_credentials.txt' # NOTE: Change this to the path of your credentials file\n",
    "process = subprocess.Popen(['twscrape', 'add_accounts', credentials, 'username:password:email:email_password:_:_'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if stderr:\n",
    "    print(stderr.decode('utf-8'))\n",
    "else:\n",
    "    print(stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "process = subprocess.Popen(['twscrape', 'login_accounts'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "if stderr:\n",
    "    print(stderr.decode('utf-8'))\n",
    "else:\n",
    "    print(stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def get_nested_value(data_dict, nested_key):\n",
    "    keys = nested_key.split(\".\")\n",
    "    for key in keys:\n",
    "        if isinstance(data_dict, dict):\n",
    "            data_dict = data_dict.get(key)\n",
    "        else:\n",
    "            return None\n",
    "    return data_dict\n",
    "\n",
    "def tweet_details(tweet_id: str):\n",
    "    \"\"\"\n",
    "    Available fields:\n",
    "    id, id_str, url, date, user.id, user.id_str, user.url, user.username, user.displayname, user.rawDescription, user.created, user.followersCount, user.friendsCount, user.statusesCount, user.favouritesCount, user.listedCount, user.mediaCount, user.location, user.profileImageUrl, user.profileBannerUrl, user.protected, user.verified, user.blue, user.blueType, user.descriptionLinks, user._type, lang, rawContent, replyCount, retweetCount, likeCount, quoteCount, conversationId, conversationIdStr, hashtags, cashtags, mentionedUsers, links, viewCount, retweetedTweet, quotedTweet, place, coordinates, inReplyToTweetId, inReplyToTweetIdStr, inReplyToUser, source, sourceUrl, sourceLabel, media.photos, media.videos, media.animated, _type\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        [\"twscrape\", \"tweet_details\", tweet_id],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    stdout, stderr = process.communicate()\n",
    "    if stderr:\n",
    "        return {\"status\": False, \"meta\": stderr, \"data\": None}\n",
    "    else:\n",
    "        tweet_data = stdout.strip().split(\"\\n\")\n",
    "        tweet_json = json.loads(tweet_data[0])\n",
    "        selected_fields = {\n",
    "            \"tweet_id\": tweet_json.get(\"id_str\"),\n",
    "            \"date\": tweet_json.get(\"date\"),\n",
    "            \"username\": get_nested_value(tweet_json, \"user.username\"),\n",
    "            \"rawContent\": tweet_json.get(\"rawContent\")\n",
    "        }\n",
    "        return {\"status\": True, \"meta\": \"success\", \"data\": selected_fields}\n",
    "\n",
    "def tweet_replies(tweet_id: str, limit=5):\n",
    "    \"\"\"\n",
    "    Available fields:\n",
    "    id, id_str, url, date, user.id, user.id_str, user.url, user.username, user.displayname, user.rawDescription, user.created, user.followersCount, user.friendsCount, user.statusesCount, user.favouritesCount, user.listedCount, user.mediaCount, user.location, user.profileImageUrl, user.profileBannerUrl, user.protected, user.verified, user.blue, user.blueType, user.descriptionLinks, user._type, lang, rawContent, replyCount, retweetCount, likeCount, quoteCount, conversationId, conversationIdStr, hashtags, cashtags, mentionedUsers, links, viewCount, retweetedTweet, quotedTweet, place, coordinates, inReplyToTweetId, inReplyToTweetIdStr, inReplyToUser.id, inReplyToUser.id_str, inReplyToUser.username, inReplyToUser.displayname, inReplyToUser._type, source, sourceUrl, sourceLabel, media.photos, media.videos, media.animated, _type\n",
    "    \"\"\"\n",
    "    limit = max(limit, 2)\n",
    "    process = subprocess.Popen(\n",
    "        [\"twscrape\", \"tweet_replies\", tweet_id, f\"--limit={limit}\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    stdout, stderr = process.communicate()\n",
    "    if stderr:\n",
    "        return {\"status\": False, \"meta\": stderr, \"data\": None}\n",
    "    else:\n",
    "        tweet_comments = stdout.strip().split(\"\\n\")\n",
    "        tweet_comments_json = [\n",
    "            {\n",
    "                \"comment_id\": comment.get(\"id_str\"),\n",
    "                \"date\": comment.get(\"date\"),\n",
    "                \"rawContent\": comment.get(\"rawContent\"),\n",
    "                \"tweet_id\": comment.get(\"inReplyToTweetIdStr\"),\n",
    "            } for comment in (json.loads(c) for c in tweet_comments)\n",
    "        ]\n",
    "        return {\"status\": True, \"meta\": \"success\", \"data\": tweet_comments_json}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "timenow = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "id_to_scrape = [\"1772965377338741248\"]\n",
    "\n",
    "replies_df = pd.DataFrame()\n",
    "tweet_df = pd.DataFrame()\n",
    "\n",
    "for tweet_id in id_to_scrape:\n",
    "    replies = tweet_replies(tweet_id, 5)\n",
    "    tweet = tweet_details(tweet_id)\n",
    "\n",
    "    if replies[\"status\"] and tweet[\"status\"]:\n",
    "        df_replies = pd.DataFrame(replies[\"data\"])\n",
    "        replies_df = pd.concat([replies_df, df_replies], ignore_index=True)\n",
    "\n",
    "        df_tweet = pd.DataFrame(tweet[\"data\"], index=[0])\n",
    "        tweet_df = pd.concat([tweet_df, df_tweet], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to scrape tweet {tweet_id}\")\n",
    "\n",
    "filename = f\"replies_{timenow}.csv\"\n",
    "replies_df.to_csv(filename, index=False)\n",
    "\n",
    "filename = f\"tweet_{timenow}.csv\"\n",
    "tweet_df.to_csv(filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
